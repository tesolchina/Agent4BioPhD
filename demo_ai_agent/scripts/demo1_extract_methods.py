#!/usr/bin/env python3
"""
Demo Script 1: Extracting Methods and Tools from Badia-i-Mompel et al. 2023
============================================================================

This script demonstrates how an AI agent can automatically analyze a scientific paper
and extract structured information about computational methods and bioinformatics tools.

Paper: "Gene regulatory network inference in the era of single-cell multi-omics"
Author: Badia-i-Mompel et al. (2023), Nature Reviews Genetics

Generated by: AI Agent
Purpose: Workshop demo for Nov 7th, 2025
"""

import re
from collections import defaultdict
import json

def extract_methods_from_paper(paper_path):
    """
    Extract computational methods mentioned in the paper.
    
    Args:
        paper_path: Path to the markdown file containing the paper
        
    Returns:
        Dictionary with categorized methods and tools
    """
    
    with open(paper_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Initialize results dictionary
    results = {
        'inference_methods': [],
        'experimental_technologies': [],
        'bioinformatics_tools': [],
        'databases': [],
        'software_packages': []
    }
    
    # Define patterns to search for
    # GRN inference methods
    grn_methods = [
        'WGCNA', 'GENIE3', 'GRNBoost2', 'SCENIC', 'PANDA', 'LIONESS',
        'ARACNe', 'CLR', 'ARACNE', 'Inferelator'
    ]
    
    # Experimental technologies
    technologies = [
        'ChIP-seq', 'ATAC-seq', 'RNA-seq', 'scRNA-seq', 'CUT&Tag',
        'DNase-seq', 'NOME-seq', 'Hi-C', 'single-cell', 'multimodal',
        'multi-omics'
    ]
    
    # Bioinformatics tools
    tools = [
        'motifmatchr', 'FIMO', 'HOMER', 'GimmeMotifs', 'MOODS',
        'Seurat', 'Scanpy', 'ArchR', 'Signac', 'SnapATAC'
    ]
    
    # Databases
    databases = [
        'JASPAR', 'TRANSFAC', 'HOCOMOCO', 'CIS-BP', 'ENCODE',
        'cisTarget', 'UniPROBE', 'GeneCards', 'GO', 'KEGG'
    ]
    
    # Search for each category
    for method in grn_methods:
        if re.search(r'\b' + re.escape(method) + r'\b', content, re.IGNORECASE):
            results['inference_methods'].append(method)
    
    for tech in technologies:
        if re.search(r'\b' + re.escape(tech) + r'\b', content, re.IGNORECASE):
            results['experimental_technologies'].append(tech)
    
    for tool in tools:
        if re.search(r'\b' + re.escape(tool) + r'\b', content, re.IGNORECASE):
            results['bioinformatics_tools'].append(tool)
    
    for db in databases:
        if re.search(r'\b' + re.escape(db) + r'\b', content, re.IGNORECASE):
            results['databases'].append(db)
    
    # Remove duplicates and sort
    for key in results:
        results[key] = sorted(list(set(results[key])))
    
    return results


def count_method_mentions(paper_path, method_list):
    """
    Count how many times each method is mentioned in the paper.
    
    Args:
        paper_path: Path to paper
        method_list: List of methods to count
        
    Returns:
        Dictionary with counts for each method
    """
    
    with open(paper_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    counts = {}
    for method in method_list:
        count = len(re.findall(r'\b' + re.escape(method) + r'\b', content, re.IGNORECASE))
        if count > 0:
            counts[method] = count
    
    # Sort by count (descending)
    return dict(sorted(counts.items(), key=lambda x: x[1], reverse=True))


def extract_key_concepts(paper_path):
    """
    Extract key concepts and their context from the paper.
    
    Args:
        paper_path: Path to paper
        
    Returns:
        Dictionary with key concepts and example contexts
    """
    
    with open(paper_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    concepts = {
        'transcription_factors': [],
        'gene_regulation': [],
        'single_cell': [],
        'chromatin_accessibility': []
    }
    
    # Extract sentences containing key concepts
    sentences = content.split('.')
    
    for sentence in sentences:
        sentence = sentence.strip()
        if 'transcription factor' in sentence.lower() and len(sentence) < 200:
            concepts['transcription_factors'].append(sentence[:150] + '...')
        if 'gene regulation' in sentence.lower() and len(sentence) < 200:
            concepts['gene_regulation'].append(sentence[:150] + '...')
        if 'single-cell' in sentence.lower() or 'single cell' in sentence.lower():
            if len(sentence) < 200:
                concepts['single_cell'].append(sentence[:150] + '...')
        if 'chromatin' in sentence.lower() and 'accessibility' in sentence.lower():
            if len(sentence) < 200:
                concepts['chromatin_accessibility'].append(sentence[:150] + '...')
    
    # Limit to first 3 examples per concept
    for key in concepts:
        concepts[key] = concepts[key][:3]
    
    return concepts


def generate_summary_report(results, counts, concepts, output_path):
    """
    Generate a formatted summary report.
    
    Args:
        results: Dictionary of extracted methods/tools
        counts: Dictionary of method mention counts
        concepts: Dictionary of key concepts with context
        output_path: Where to save the report
    """
    
    report = []
    report.append("=" * 80)
    report.append("AUTOMATED PAPER ANALYSIS REPORT")
    report.append("Paper: Gene regulatory network inference in the era of single-cell multi-omics")
    report.append("Authors: Badia-i-Mompel et al. (2023)")
    report.append("Generated by: AI Agent")
    report.append("=" * 80)
    report.append("")
    
    # Section 1: Methods and Tools
    report.append("1. COMPUTATIONAL METHODS IDENTIFIED")
    report.append("-" * 80)
    for category, items in results.items():
        if items:
            report.append(f"\n{category.replace('_', ' ').title()}:")
            for item in items:
                mention_count = counts.get(item, 0)
                report.append(f"  • {item} (mentioned {mention_count} times)")
    
    report.append("\n")
    report.append("2. TOP 10 MOST MENTIONED METHODS/TOOLS")
    report.append("-" * 80)
    top_methods = list(counts.items())[:10]
    for i, (method, count) in enumerate(top_methods, 1):
        report.append(f"{i:2d}. {method:25s} - {count:3d} mentions")
    
    report.append("\n")
    report.append("3. KEY CONCEPTS AND CONTEXT")
    report.append("-" * 80)
    for concept, examples in concepts.items():
        if examples:
            report.append(f"\n{concept.replace('_', ' ').title()}:")
            for i, example in enumerate(examples, 1):
                report.append(f"  {i}. {example}")
    
    report.append("\n")
    report.append("4. RECOMMENDATIONS FOR WORKSHOP")
    report.append("-" * 80)
    report.append("  • Focus on ATAC-seq + RNA-seq integration (highly relevant)")
    report.append("  • Demonstrate motifmatchr for TF binding site prediction")
    report.append("  • Show how to combine expression and accessibility data")
    report.append("  • Discuss limitations of different GRN inference methods")
    report.append("  • Highlight importance of benchmarking")
    
    report.append("\n")
    report.append("=" * 80)
    report.append("END OF REPORT")
    report.append("=" * 80)
    
    # Write to file
    with open(output_path, 'w') as f:
        f.write('\n'.join(report))
    
    print(f"Report saved to: {output_path}")
    print("\nSummary:")
    print(f"  - {len(results['inference_methods'])} GRN inference methods identified")
    print(f"  - {len(results['bioinformatics_tools'])} bioinformatics tools found")
    print(f"  - {len(results['databases'])} databases referenced")
    print(f"  - {len(results['experimental_technologies'])} experimental technologies mentioned")


if __name__ == "__main__":
    # Paths
    paper_path = "/workspaces/Agent4BioPhD/docs/Badia-i-Mompel et al 2023.md"
    output_path = "/workspaces/Agent4BioPhD/outputs/paper_analysis_report.txt"
    
    print("Starting automated paper analysis...")
    print("=" * 80)
    
    # Extract methods and tools
    print("\n[1/4] Extracting methods and tools...")
    results = extract_methods_from_paper(paper_path)
    
    # Count mentions
    print("[2/4] Counting method mentions...")
    all_items = []
    for items in results.values():
        all_items.extend(items)
    counts = count_method_mentions(paper_path, all_items)
    
    # Extract key concepts
    print("[3/4] Extracting key concepts...")
    concepts = extract_key_concepts(paper_path)
    
    # Generate report
    print("[4/4] Generating summary report...")
    generate_summary_report(results, counts, concepts, output_path)
    
    # Also save as JSON for programmatic access
    json_output = output_path.replace('.txt', '.json')
    with open(json_output, 'w') as f:
        json.dump({
            'methods_and_tools': results,
            'mention_counts': counts,
            'key_concepts': concepts
        }, f, indent=2)
    
    print(f"JSON data saved to: {json_output}")
    print("\nDone! ✓")
